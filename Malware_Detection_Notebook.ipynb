import pandas as pd
import io

# Provided data
data = """
Feature1,Feature2,Label
0.8747783776199903,0.27113982193597436,A
-0.5942469043813675,-0.9879512062438888,B
-0.08233577509166534,-1.6225966196832866,A
-0.43303149508204863,-2.4899388846123642,A
0.4384396954593156,2.1043865430536157,B
-1.3252026256146157,-0.15631142345044205,A
0.6320093948367899,0.1340590879481499,A
-1.4956631951958126,0.46668939361347117,B
0.39341004211008385,-0.04575243094358256,A
-0.008680715553340244,-0.8620325700333963,B
-0.9030295665868056,-0.211036090736846,A
0.2212345077771091,0.80575872177453,A
-1.1358364542835462,0.1126771766643193,B
0.31680546424520656,0.2440847396852225,A
1.201843371214949,0.2576619633008928,B
0.4453028355516185,0.4734071932144127,B
-0.9826012052564095,0.7798732435174515,A
0.025322509079159432,-1.2349782444451278,A
0.06984021163823735,1.3225961353536342,B
"""

# Load the dataset into a pandas DataFrame
df = pd.read_csv(io.StringIO(data))

# Display the first few rows of the dataset
df.head()
Feature1	Feature2	Label
0	0.874778	0.271140	A
1	-0.594247	-0.987951	B
2	-0.082336	-1.622597	A
3	-0.433031	-2.489939	A
4	0.438440	2.104387	B
# Data Exploration
print("Dataset shape:", df.shape)
print("Columns:", df.columns)
print("\nData types:")
print(df.dtypes)
print("\nSummary statistics:")
print(df.describe())

# Check for missing values
print("\nMissing values:")
print(df.isnull().sum())

# Check for duplicates
print("\nDuplicate rows:")
print(df.duplicated().sum())

# Visualize the distribution of the target variable
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='Label', data=df)
plt.title('Distribution of Labels')
plt.show()
Dataset shape: (19, 3)
Columns: Index(['Feature1', 'Feature2', 'Label'], dtype='object')

Data types:
Feature1    float64
Feature2    float64
Label        object
dtype: object

Summary statistics:
        Feature1   Feature2
count  19.000000  19.000000
mean   -0.123244  -0.033593
std     0.768563   1.060235
min    -1.495663  -2.489939
25%    -0.748638  -0.536534
50%     0.025323   0.134059
75%     0.415925   0.470048
max     1.201843   2.104387

Missing values:
Feature1    0
Feature2    0
Label       0
dtype: int64

Duplicate rows:
0

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Preprocessing
# Convert categorical variable 'Label' to numerical format
label_encoder = LabelEncoder()
df['Label'] = label_encoder.fit_transform(df['Label'])

# Split the dataset into features (X) and the target variable (y)
X = df.drop('Label', axis=1)
y = df['Label']

# Split the data into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model Building
# Initialize and train a Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)

# Model Evaluation
# Predict on the testing set
y_pred = rf_classifier.predict(X_test)

# Evaluate model performance
print("Classification Report:")
print(classification_report(y_test, y_pred))
Classification Report:
              precision    recall  f1-score   support

           0       0.50      0.20      0.29         5
           1       0.00      0.00      0.00         1

    accuracy                           0.17         6
   macro avg       0.25      0.10      0.14         6
weighted avg       0.42      0.17      0.24         6

from sklearn.model_selection import GridSearchCV

# Define hyperparameters grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Perform grid search with 5-fold cross-validation
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Use the best model to make predictions
best_rf_model = grid_search.best_estimator_
y_pred = best_rf_model.predict(X_test)

# Evaluate model performance
print("Classification Report:")
print(classification_report(y_test, y_pred))
Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}
Classification Report:
              precision    recall  f1-score   support

           0       0.67      0.40      0.50         5
           1       0.00      0.00      0.00         1

    accuracy                           0.33         6
   macro avg       0.33      0.20      0.25         6
weighted avg       0.56      0.33      0.42         6

import matplotlib.pyplot as plt

# Get feature importances
feature_importances = best_rf_model.feature_importances_

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.barh(X.columns, feature_importances)
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance Plot')
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Get predicted probabilities for the positive class
y_probs = best_rf_model.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC score
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc_score = roc_auc_score(y_test, y_probs)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt

# Get feature importances
feature_importances = best_rf_model.feature_importances_

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.barh(X.columns, feature_importances)
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance Plot')
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Get predicted probabilities for the positive class
y_probs = best_rf_model.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC score
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
auc_score = roc_auc_score(y_test, y_probs)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {auc_score:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report

# Print the classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))
Classification Report:
              precision    recall  f1-score   support

           0       0.67      0.40      0.50         5
           1       0.00      0.00      0.00         1

    accuracy                           0.33         6
   macro avg       0.33      0.20      0.25         6
weighted avg       0.56      0.33      0.42         6

import numpy as np
import matplotlib.pyplot as plt

# Extract feature importances from the best RF model
feature_importances = best_rf_model.feature_importances_

# Sort feature importances in descending order
indices = np.argsort(feature_importances)[::-1]

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X_train.shape[1]), feature_importances[indices], color="b", align="center")
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.xlim([-1, X_train.shape[1]])
plt.tight_layout()
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate precision, recall, and F1-score for each class
precision = precision_score(y_test, y_pred, average=None)
recall = recall_score(y_test, y_pred, average=None)
f1 = f1_score(y_test, y_pred, average=None)

# Create a DataFrame to display the metrics
metrics_df = pd.DataFrame({'Precision': precision, 'Recall': recall, 'F1-Score': f1}, index=label_encoder.classes_)

print("Performance Metrics Analysis:")
print(metrics_df)
Performance Metrics Analysis:
   Precision  Recall  F1-Score
A   0.666667     0.4       0.5
B   0.000000     0.0       0.0
from sklearn.model_selection import GridSearchCV

# Define hyperparameters grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Perform grid search with 5-fold cross-validation
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Use the best model to make predictions
best_rf_model = grid_search.best_estimator_
y_pred = best_rf_model.predict(X_test)

# Evaluate model performance
print("\nUpdated Performance Metrics Analysis:")
precision = precision_score(y_test, y_pred, average=None)
recall = recall_score(y_test, y_pred, average=None)
f1 = f1_score(y_test, y_pred, average=None)
metrics_df = pd.DataFrame({'Precision': precision, 'Recall': recall, 'F1-Score': f1}, index=label_encoder.classes_)
print(metrics_df)
Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}

Updated Performance Metrics Analysis:
   Precision  Recall  F1-Score
A   0.666667     0.4       0.5
B   0.000000     0.0       0.0
print("Best Hyperparameters:", grid_search.best_params_)
Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}
# Use the best model to make predictions
best_rf_model = grid_search.best_estimator_
y_pred = best_rf_model.predict(X_test)

# Evaluate model performance
print("\nUpdated Performance Metrics Analysis:")
precision = precision_score(y_test, y_pred, average=None)
recall = recall_score(y_test, y_pred, average=None)
f1 = f1_score(y_test, y_pred, average=None)
metrics_df = pd.DataFrame({'Precision': precision, 'Recall': recall, 'F1-Score': f1}, index=label_encoder.classes_)
print(metrics_df)
Updated Performance Metrics Analysis:
   Precision  Recall  F1-Score
A   0.666667     0.4       0.5
B   0.000000     0.0       0.0
from sklearn.metrics import confusion_matrix

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Display the confusion matrix
print("Confusion Matrix:")
print(conf_matrix)
Confusion Matrix:
[[2 3]
 [1 0]]
# Get the indices of misclassified instances
misclassified_indices = (y_test != y_pred)

# Extract the feature values of misclassified instances
misclassified_features = X_test[misclassified_indices]

# Display the feature values of misclassified instances
print("Misclassified Instances Features:")
print(misclassified_features)
Misclassified Instances Features:
    Feature1  Feature2
0   0.874778  0.271140
11  0.221235  0.805759
1  -0.594247 -0.987951
16 -0.982601  0.779873
# Get the indices of correctly classified instances
correct_indices = (y_test == y_pred)

# Extract the feature values of correctly classified instances
correct_features = X_test[correct_indices]

# Print the mean of features for correctly classified instances
print("Mean of Correctly Classified Instances Features:")
print(correct_features.mean())

# Print the mean of features for misclassified instances
print("\nMean of Misclassified Instances Features:")
print(misclassified_features.mean())
Mean of Correctly Classified Instances Features:
Feature1   -0.465896
Feature2   -0.101032
dtype: float64

Mean of Misclassified Instances Features:
Feature1   -0.120209
Feature2    0.217205
dtype: float64
import matplotlib.pyplot as plt

# Plot the feature values of correctly classified instances
plt.scatter(correct_features['Feature1'], correct_features['Feature2'], color='green', label='Correctly Classified')

# Plot the feature values of misclassified instances
plt.scatter(misclassified_features['Feature1'], misclassified_features['Feature2'], color='red', label='Misclassified')

# Plot the mean feature values for correctly classified instances
plt.scatter(correct_features.mean()['Feature1'], correct_features.mean()['Feature2'], color='blue', marker='x', label='Mean Correctly Classified')

# Plot the mean feature values for misclassified instances
plt.scatter(misclassified_features.mean()['Feature1'], misclassified_features.mean()['Feature2'], color='black', marker='x', label='Mean Misclassified')

# Add labels and legend
plt.xlabel('Feature1')
plt.ylabel('Feature2')
plt.title('Comparison of Feature Values')
plt.legend()

# Show the plot
plt.show()

from scipy.stats import ttest_ind

# Perform a t-test for Feature1 between correctly classified and misclassified instances
t_statistic_feature1, p_value_feature1 = ttest_ind(correct_features['Feature1'], misclassified_features['Feature1'])

# Perform a t-test for Feature2 between correctly classified and misclassified instances
t_statistic_feature2, p_value_feature2 = ttest_ind(correct_features['Feature2'], misclassified_features['Feature2'])

print("T-Test Results:")
print("Feature1:")
print("   T-Statistic:", t_statistic_feature1)
print("   P-Value:", p_value_feature1)
print("Feature2:")
print("   T-Statistic:", t_statistic_feature2)
print("   P-Value:", p_value_feature2)
T-Test Results:
Feature1:
   T-Statistic: -0.4236009276487386
   P-Value: 0.6936421077821694
Feature2:
   T-Statistic: -0.5042322816325594
   P-Value: 0.6406055583646098
import matplotlib.pyplot as plt

# Extract feature importances from the best RandomForest model
feature_importances = best_rf_model.feature_importances_

# Sort feature importances in descending order
indices = np.argsort(feature_importances)[::-1]

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), feature_importances[indices], align="center")
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.xlabel("Feature")
plt.ylabel("Importance Score")
plt.tight_layout()
plt.show()

# Extract misclassified instances
misclassified_indices = y_test != y_pred
misclassified_X = X_test[misclassified_indices]

# Print misclassified instances features
print("Misclassified Instances Features:")
print(misclassified_X)
Misclassified Instances Features:
    Feature1  Feature2
0   0.874778  0.271140
11  0.221235  0.805759
1  -0.594247 -0.987951
16 -0.982601  0.779873
from scipy.stats import ttest_ind

# Extract correctly classified instances
correctly_classified_indices = y_test == y_pred
correctly_classified_X = X_test[correctly_classified_indices]

# Calculate means of correctly classified instances features
correct_mean_feature1 = correctly_classified_X['Feature1'].mean()
correct_mean_feature2 = correctly_classified_X['Feature2'].mean()

# Calculate means of misclassified instances features
misclassified_mean_feature1 = misclassified_X['Feature1'].mean()
misclassified_mean_feature2 = misclassified_X['Feature2'].mean()

# Perform t-test for Feature1
t_statistic_feature1, p_value_feature1 = ttest_ind(correctly_classified_X['Feature1'], misclassified_X['Feature1'])

# Perform t-test for Feature2
t_statistic_feature2, p_value_feature2 = ttest_ind(correctly_classified_X['Feature2'], misclassified_X['Feature2'])

# Output t-test results
print("T-Test Results:")
print(f"Feature1:\n   T-Statistic: {t_statistic_feature1}\n   P-Value: {p_value_feature1}")
print(f"Feature2:\n   T-Statistic: {t_statistic_feature2}\n   P-Value: {p_value_feature2}")
T-Test Results:
Feature1:
   T-Statistic: -0.4236009276487386
   P-Value: 0.6936421077821694
Feature2:
   T-Statistic: -0.5042322816325594
   P-Value: 0.6406055583646098
# Calculate the mean of feature values for correctly classified instances
correctly_classified_means = X_test[y_test == y_pred].mean()
print("Mean of Correctly Classified Instances Features:")
print(correctly_classified_means)
Mean of Correctly Classified Instances Features:
Feature1   -0.465896
Feature2   -0.101032
dtype: float64
# Calculate the mean of feature values for misclassified instances
misclassified_means = X_test[y_test != y_pred].mean()
print("\nMean of Misclassified Instances Features:")
print(misclassified_means)
Mean of Misclassified Instances Features:
Feature1   -0.120209
Feature2    0.217205
dtype: float64
# Output the mean feature values for both correctly classified and misclassified instances
print("\nMean of Correctly Classified Instances Features:")
print(correctly_classified_means)
print("\nMean of Misclassified Instances Features:")
print(misclassified_means)
Mean of Correctly Classified Instances Features:
Feature1   -0.465896
Feature2   -0.101032
dtype: float64

Mean of Misclassified Instances Features:
Feature1   -0.120209
Feature2    0.217205
dtype: float64
# Perform a t-test to compare means of correctly and misclassified instances for each feature
from scipy.stats import ttest_ind

# T-test for Feature1
t_statistic_feat1, p_value_feat1 = ttest_ind(X_test[y_test == y_pred]['Feature1'], X_test[y_test != y_pred]['Feature1'])
print("Feature1:")
print("   T-Statistic:", t_statistic_feat1)
print("   P-Value:", p_value_feat1)

# T-test for Feature2
t_statistic_feat2, p_value_feat2 = ttest_ind(X_test[y_test == y_pred]['Feature2'], X_test[y_test != y_pred]['Feature2'])
print("Feature2:")
print("   T-Statistic:", t_statistic_feat2)
print("   P-Value:", p_value_feat2)
Feature1:
   T-Statistic: -0.4236009276487386
   P-Value: 0.6936421077821694
Feature2:
   T-Statistic: -0.5042322816325594
   P-Value: 0.6406055583646098
Malware Detection Project
This notebook aims to develop a machine learning model for detecting malware based on selected features. The dataset contains features extracted from malware samples, along with their corresponding labels.

Data Preprocessing
Loaded the dataset and handled missing values.
Encoded categorical variables using one-hot encoding.
Split the data into features (X) and the target variable (y).
Split the data into training and testing sets.
Model Building and Evaluation
Trained a Random Forest classifier on the training data.
Evaluated the model's performance using a classification report.
Hyperparameter Tuning: Performed grid search with cross-validation to find the best hyperparameters.
Visualizations and Analysis
Feature Importance Visualization
```python

Feature Importance Visualization
(Insert code for plotting feature importance here)
ROC Curve and AUC Score
(Insert code for plotting ROC curve and calculating AUC score here)
Confusion Matrix
(Insert code for plotting confusion matrix here)
Print mean of correctly classified instances features
print("Mean of Correctly Classified Instances Features:") print(df_correct.mean())

Print mean of misclassified instances features
print("Mean of Misclassified Instances Features:") print(df_misclassified.mean())

T-Test Results
print("T-Test Results:") print("Feature1:") print(" T-Statistic:", t_statistic_feature1) print(" P-Value:", p_value_feature1) print("Feature2:") print(" T-Statistic:", t_statistic_feature2) print(" P-Value:", p_value_feature2)

